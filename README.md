<div align="center">

# ğŸ·ï¸ [Label Anything](https://pasqualedem.github.io/LabelAnything/)

### Multi-Class Few-Shot Semantic Segmentation with Visual Prompts

[![Project Page](https://img.shields.io/badge/ğŸŒ_Project-Page-blue.svg)](https://pasqualedem.github.io/LabelAnything/)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/label-anything-multi-class-few-shot-semantic/few-shot-semantic-segmentation-on-coco-20i-2-1)](https://paperswithcode.com/sota/few-shot-semantic-segmentation-on-coco-20i-2-1?p=label-anything-multi-class-few-shot-semantic)
[![arXiv](https://img.shields.io/badge/arXiv-2407.02075-b31b1b.svg)](https://arxiv.org/abs/2407.02075)
[![ECAI 2025](https://img.shields.io/badge/ECAI-2025-brightgreen.svg)](https://ecai2025.org/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

</div>

---

## ğŸŒŸ Overview

**Label Anything** is a novel method for multi-class few-shot semantic segmentation using visual prompts. This repository contains the official implementation of our ECAI 2025 paper, enabling precise segmentation with just a few prompted examples.

<div align="center">

![Label Anything Demo](assets/la.png)

*Visual prompting meets few-shot learning with a new fast and efficient architecture.*

</div>

## ğŸš€ Quick Start

### âš¡ One-Line Demo

Experience Label Anything instantly with our streamlined demo:

```bash
uvx --from git+https://github.com/pasqualedem/LabelAnything app
```

> **ğŸ’¡ Pro Tip**: This command uses [uv](https://docs.astral.sh/uv/) for lightning-fast package management and execution.

### ğŸ› ï¸ Manual Installation

For development and customization:

```bash
# Clone the repository
git clone https://github.com/pasqualedem/LabelAnything.git
cd LabelAnything

# Create virtual environment with uv
uv sync
source .venv/bin/activate
```

> **âš ï¸ System Requirements**: Linux environment with CUDA 12.1 support

## ğŸ“¦ Pre-trained Models

Access our collection of state-of-the-art checkpoints:

<div align="center">

| ğŸ§  Encoder | ğŸ“ Embedding Size | ğŸ–¼ï¸ Image Size | ğŸ“ Fold | ğŸ”— Checkpoint |
|------------|-------------------|----------------|----------|---------------|
| **SAM** | 512 | 1024 | - | [![HF](https://img.shields.io/badge/ğŸ¤—_HuggingFace-Model-FFD21E?style=for-the-badge)](https://huggingface.co/pasqualedem/label_anything_sam_1024_coco) |
| **ViT-MAE** | 256 | 480 | - | [![HF](https://img.shields.io/badge/ğŸ¤—_HuggingFace-Model-FFD21E?style=for-the-badge)](https://huggingface.co/pasqualedem/label_anything_mae_480_coco) |
| **ViT-MAE** | 256 | 480 | 0 | [![HF](https://img.shields.io/badge/ğŸ¤—_HuggingFace-Model-FFD21E?style=for-the-badge)](https://huggingface.co/pasqualedem/label_anything_coco_fold0_mae_7a5p0t63) |

</div>

### ğŸ”Œ Model Loading

```python
from label_anything.models import LabelAnything

# Load pre-trained model
model = LabelAnything.from_pretrained("pasqualedem/label_anything_sam_1024_coco")
```

## ğŸ¯ Training Pipeline

### ğŸ“Š Dataset Setup: COCO 2017

Prepare the COCO dataset with our automated setup:

```bash
# Navigate to data directory
cd data && mkdir coco && cd coco

# Download COCO 2017 images and 2014 annotations
wget http://images.cocodataset.org/zips/train2017.zip
wget http://images.cocodataset.org/zips/val2017.zip
wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip

# Extract and organize
unzip "*.zip" && rm *.zip
mv val2017/* train2017/ && mv train2017 train_val_2017 && rm -rf val2017
```

### ğŸ”§ Annotation Preprocessing

Synchronize filenames between images and annotations:

```bash
python main.py rename_coco20i_json --instances_path data/coco/annotations/instances_train2014.json
python main.py rename_coco20i_json --instances_path data/coco/annotations/instances_val2014.json
```

### ğŸ§  Feature Extraction

#### SAM Encoder Setup
```bash
# Download SAM checkpoint
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth

# Extract embeddings (optional but recommended for speed)
mkdir -p data/coco/vit_sam_embeddings/{last_hidden_state,last_block_state}
python main.py generate_embeddings \
  --encoder vit_b \
  --checkpoint checkpoints/sam_vit_b_01ec64.pth \
  --use_sam_checkpoint \
  --directory data/coco/train_val_2017 \
  --batch_size 16 \
  --num_workers 8 \
  --outfolder data/coco/vit_sam_embeddings/last_hidden_state \
  --last_block_dir data/coco/vit_sam_embeddings/last_block_state \
  --custom_preprocess
```

#### ViT-MAE Encoders
```bash
# Base ViT-MAE (1024px)
python main.py generate_embeddings \
  --encoder vit_b_mae \
  --directory data/coco/train_val_2017 \
  --batch_size 32 \
  --outfolder data/coco/embeddings_vit_mae_1024/ \
  --model_name facebook/vit-mae-base \
  --image_resolution 1024 \
  --huggingface

# Base ViT-MAE (480px)
python main.py generate_embeddings \
  --encoder vit_b_mae \
  --directory data/coco/train_val_2017 \
  --batch_size 64 \
  --outfolder data/coco/embeddings_vit_mae_480 \
  --model_name facebook/vit-mae-base \
  --image_resolution 480 \
  --huggingface

# Large ViT-MAE (480px)
python main.py generate_embeddings \
  --encoder vit_l_mae \
  --directory data/coco/train_val_2017 \
  --batch_size 64 \
  --outfolder data/coco/embeddings_vit_mae_l_480 \
  --model_name facebook/vit-mae-large \
  --image_resolution 480 \
  --huggingface

# DinoV3
python main.py generate_embeddings \
  --directory data/coco/train_val_2017 \
  --batch_size 64 \
  --outfolder data/coco/embeddings_dinov3_480 \
  --model_name facebook/dinov3-vitb16-pretrain-lvd1689m \
  --image_resolution 480 \
  --huggingface
```

### ğŸ‹ï¸ Training & Evaluation

#### Single GPU Training
```bash
# Train with pre-extracted embeddings
python main.py experiment --parameters="parameters/trainval/coco20i/mae.yaml"

# Train without pre-extracted embeddings
python main.py experiment --parameters="parameters/trainval/coco20i/mae_noembs.yaml"
```

#### Multi-GPU Training
```bash
# Accelerated training for faster convergence
accelerate launch --multi_gpu main.py experiment --parameters="parameters/trainval/coco20i/mae.yaml"
```

> **ğŸ“ˆ Experiment Tracking**: All experiments are automatically logged to [Weights & Biases](https://wandb.ai/site). Results are saved in `offline/wandb/run-<date>-<run_id>/files/`.

## ğŸ—ï¸ Project Architecture

```
ğŸ“¦ LabelAnything
    ğŸŒŸ Core Components
    â”œâ”€â”€ label_anything/          # ğŸ”§ Main codebase
    â”‚   â”œâ”€â”€ **main**.py          # ğŸšª CLI entry point
    â”‚   â”œâ”€â”€ cli.py               # ğŸ’» Command interface
    â”‚   â”œâ”€â”€ data/                # ğŸ“Š Dataset handling
    â”‚   â”œâ”€â”€ demo/                # ğŸ® Interactive demos
    â”‚   â”œâ”€â”€ experiment/          # ğŸ§ª Training workflows
    â”‚   â”œâ”€â”€ models/              # ğŸ¤– Neural architectures
    â”‚   â”œâ”€â”€ loss/                # ğŸ“‰ Loss functions
    â”‚   â””â”€â”€ utils/               # ğŸ› ï¸ Utilities
    â””â”€â”€ parameters/              # âš™ï¸ Configuration files
        â”œâ”€â”€ trainval/            # ğŸ“š Training configs
        â”œâ”€â”€ validation/          # ğŸ“– Validation configs
        â””â”€â”€ test/                # ğŸ§ª Testing configs

    ğŸ“š Resources
    â”œâ”€â”€ notebooks/               # ğŸ““ Analysis & demos
    â”œâ”€â”€ assets/                  # ğŸ–¼ï¸ Media files
    â”œâ”€â”€ data/                    # ğŸ’¾ Dataset storage
    â””â”€â”€ checkpoints/             # ğŸ† Model weights

    ğŸš€ Deployment
    â”œâ”€â”€ slurm/                   # âš¡ HPC job scripts
    â””â”€â”€ app.py                   # ğŸŒ Web application
```

## ğŸ¨ Key Features

- **ğŸ¯ Few-Shot Learning**: Achieve remarkable results with minimal training data
- **ğŸ–¼ï¸ Visual Prompting**: Intuitive interaction through visual cues
- **âš¡ Multi-GPU Support**: Accelerated training on modern hardware
- **ğŸ”„ Cross-Validation**: Robust 4-fold evaluation protocol
- **ğŸ“Š Rich Logging**: Comprehensive experiment tracking
- **ğŸ¤— HuggingFace Integration**: Seamless model sharing and deployment

## ğŸ“„ Citation

If you find Label Anything useful in your research, please cite our work:

```bibtex
@inproceedings{labelanything2025,
  title={LabelAnything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts},
  author={De Marinis, Pasquale and Fanelli, Nicola and Scaringi, Raffaele and Colonna, Emanuele and Fiameni, Giuseppe and Vessio, Gennaro and Castellano, Giovanna},
  booktitle={ECAI 2025},
  year={2025}
}
```

## ğŸ¤ Contributing

We welcome contributions! Feel free to:
- ğŸ› Report bugs by opening an issue
- ğŸ’¡ Suggest new features or improvements
- ğŸ”§ Submit pull requests with bug fixes or enhancements
- ğŸ“š Improve documentation and examples

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Made with â¤ï¸ by the CilabUniba Label Anything Team**

</div>
