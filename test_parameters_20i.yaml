experiment:
  # It contains all the about the grids and the group of runs:
  name: LabelAnything # name of the logger platform experiment
  group: Test # name of group of experiments for the logger platform
  continue_with_errors: False # continue with other runs even if a run fails
  start_from_grid: 0 # skip grids in the grid search
  start_from_run: 0 # skip runs from the selected grid
  search: grid
  # n_trials: 5

parameters:
  # Contains the parameters to build the grid.
  # Each value should be a dict or a list

  logger:
    log_frequency: [1]
    tags: [[Test]] # list of tags to attach to the run in logger platform
    train_image_log_frequency: [5]
    val_image_log_frequency: [5]
    experiment_save_delta: [null] # save experiment every n seconds
    tmp_dir: [tmp]
    wandb:
      offline: [False]
      entity: [cilabuniba]
      # resume: [True]
      # run_id: [pg70tf8u]

  train_params:
    loss:
      class_weighting: [True]
      components:
        # - focal:
        #     weight: 0.485
        #   dice:
        #     weight: 0.05
        #   rmi:
        #     weight: 0.485
        - focal:
            weight: 0.5
          fp:
            weight: 0.025
          prompt_contrastive:
            weight: 0.475
    seed: &seed [42] # random seed to set
    # substitution_threshold: [0.1] # threshold
    num_points: [1] # number of points for class after the substitution
    max_epochs: [4]
    compile: [False]
    initial_lr: [0.00001]
    backbone_lr: [0.00001]
    optimizer: [AdamW]
    scheduler:
      type: [cosine]
      step_moment: [batch]
      num_warmup_steps: [1000]
      # warmup_steps: [2500] 
    substitute: [False]
    accumulate_substitution: [False] # Perform gradient accumulation over the substitution sub-batch
    validation_reruns: [2]
    watch_metric: [loss] # metric to watch for early stopping and scheduler (loss or miou)
    freeze_backbone: [False]
    check_nan: [1] # check for nan every n batches

  model:
    name: [lam_mae_b] # path to model class or model name contained in EzDL or super-gradients
    spatial_convs: [3]
    class_attention: [True]
    example_attention: [True]
    image_embed_dim: [768]
    embed_dim: &embed_dim [256]
    image_size: &image_size [480]
    few_type: [Prototype]
    segment_example_logits: [True]
    transformer_feature_size: [30]
    class_fusion: [sigmoid]
    class_embedding_dim: [1024]
    class_encoder:
      name: [RandomMatrixEncoder]
      bank_size: [100]
      embed_dim: *embed_dim


  dataset: # parameters depending on the class you defined for the dataset
    datasets:
      coco20i:
        name: [coco]
        instances_path: [data/annotations/instances_train2014.json]
        img_dir: &img_dir [data/images/train2017]
        split: ["train"]
        val_fold_idx: [0]
        remove_small_annotations: [True]
        n_folds: [4]
      val_coco20i_N1K1: &val_coco20i_N1K1
        name: [coco]
        instances_path: [data/annotations/instances_val2014.json]
        img_dir: *img_dir
        split: ["val"]
        val_fold_idx: [0]
        n_folds: [4]
        n_shots: [1]
        n_ways: [1]
        do_subsample: [False]
        add_box_noise: [False]
      val_coco20i_N2K1:
        <<: *val_coco20i_N1K1
        n_ways: [2]
    common:
      do_subsample: [True]
      add_box_noise: [True]
      max_points_annotations: [50]
      max_points_per_annotation: [10]
      image_size: *image_size
  dataloader:
    num_workers: [0]
    possible_batch_example_nums: [[[2, 2, 1]]]
    val_possible_batch_example_nums: [[[1, 1]]] 
    prompt_types: [["point", "bbox", "mask"]]
    val_prompt_types: [["mask"]]

other_grids:
