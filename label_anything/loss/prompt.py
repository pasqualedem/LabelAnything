import torch
import torch.nn as nn

from torch.nn.functional import normalize, binary_cross_entropy
from einops import rearrange

from label_anything.data.utils import BatchKeys
from label_anything.loss.utils import loss_orthogonality
from label_anything.utils.utils import LossDict, ResultDict


class PromptContrastiveLoss(nn.Module):
    def __init__(self):
        """
        Computes the contrastive loss of the class prompts generated for each example in the support set. 
        """
        super().__init__()
        self.t_prime = nn.Parameter(torch.tensor([torch.log(torch.tensor(10))]))
        self.bias = nn.Parameter(torch.tensor([-10.0]))
        
    def forward(self, result):
        """
        Arguments:
            class_embeddings (torch.Tensor): the class embeddings generated by the model (B x M x C x D)
        """
        class_embeddings = result[ResultDict.EXAMPLES_CLASS_EMBS]
        flag_examples = result[BatchKeys.FLAG_EXAMPLES]
        
        B, M, C, D = class_embeddings.shape
        
        flags = rearrange(flag_examples, "b m c -> b (m c) 1")
        valid_elements = (flags > 0).sum(dim=1) # B x 1
        flags = (~(flags.repeat(1, 1, M*C).bool())).float()
        flags = (~(flags + rearrange(flags, " b c d -> b d c")).bool())
        flags = torch.triu(flags, diagonal=1)
        
        class_embeddings = rearrange(class_embeddings, "b m c d -> b (m c) d")
        class_embeddings = normalize(class_embeddings, p=2, dim=-1)
        dot_products = class_embeddings @ rearrange(class_embeddings, " b c d -> b d c")
        dot_products = dot_products * torch.exp(self.t_prime) + self.bias
        
        contrastive_matrix = torch.eye(C, device=class_embeddings.device)
        contrastive_matrix = contrastive_matrix.unsqueeze(0).repeat(B, M, M)
        contrastive_matrix = 2 * contrastive_matrix - 1
        loss = -torch.log(torch.sigmoid(dot_products * contrastive_matrix))
        return (loss / valid_elements.unsqueeze(2))[flags].sum() / B
    

class ClassEmbeddingContrastiveLoss(nn.Module):
    def forward(self, result):
        class_embeddings = result[ResultDict.EXAMPLES_CLASS_EMBS]
        class_embeddings = rearrange(class_embeddings, "b n c d -> b (n c) d")
        return loss_orthogonality(class_embeddings)
        
        
        
        
